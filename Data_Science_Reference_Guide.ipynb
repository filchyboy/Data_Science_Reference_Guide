{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Data Science Reference Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pmKaeIqWdZuC"
   },
   "source": [
    "##The DRY Principle: Don't Repeat Yourself\n",
    "\n",
    "DRY stand for \"Don't Repeat Yourself,\" a basic principle of software development aimed at reducing repetition of information. The DRY principle is stated as, \"Every piece of knowledge or logic must have a single, unambiguous representation within a system.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A9qQSxbKIA9r"
   },
   "source": [
    "###Loading Libraries\n",
    "\n",
    "Load libraries first to help out the viewer so they know upfront what kind of stuff will be done within the context of your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NeoAFLh_Dnny"
   },
   "outputs": [],
   "source": [
    "# Library imports to run library functions\n",
    "import numpy as np \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gXMatSEOBDyZ"
   },
   "source": [
    "##Importing Data & Cleaning It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M93XegIhBJgk"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(url_of_data, header=None, names=column_headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vw8TL7VEBsWn"
   },
   "source": [
    "pd.read_csv is where I'd want to read doc notes. Highlights include arguments passed which disallows using top row as column headers (header=) and (names=) which allows you to insert column headers upon import. Useful.\n",
    "\n",
    "To read docs you can highlight over the command, look it up on Google, or use this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HQNTHzl1H4Fk"
   },
   "outputs": [],
   "source": [
    "?pd.read_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ytt5j6GECtCH"
   },
   "source": [
    "###Scoping Details of Data\n",
    "\n",
    "(exclude='number') or (include='all) for df.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pnQUz72nChkB"
   },
   "outputs": [],
   "source": [
    "df.shape\n",
    "df.head()\n",
    "df.types\n",
    "df.info()\n",
    "df.describe()\n",
    "####Counting & sorting within a column\n",
    "df['column'].value_counts() #count the number of values in the column\n",
    "df['column'].value_counts().nlargest(15) #find value counts, sort, and choose how many to display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###NULLS or Why Isnull versus Isna (They do the same thing!)\n",
    "\n",
    "But why have two methods with different names do the same thing?\n",
    "This is because pandas' DataFrames are based on R's DataFrames. In R na and null are two separate things. Read this post for more information.\n",
    "\n",
    "However, in python, pandas is built on top of numpy, which has neither na nor null values. Instead numpy has NaN values (which stands for \"Not a Number\"). Consequently, pandas also uses NaN values.\n",
    "\n",
    "In short\n",
    "To detect NaN values numpy uses np.isnan().\n",
    "\n",
    "To detect NaN values pandas uses either .isna() or .isnull().\n",
    "The NaN values are inherited from the fact that pandas is built on top of numpy, while the two functions' names originate from R's DataFrames, whose structure and functionality pandas tried to mimic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Nulls\n",
    "df.isnull().sum() #takes a scalar or array-like object and indictates whether values are missing\n",
    "df.isna().sum() #returns a boolean same-sized object indicating if the values are NA\n",
    "####Resolving nulls\n",
    "df.fillna()\n",
    "df.dropna()\n",
    "df.drop()\n",
    "df = df.drop(columns=['Column1', 'Column2', 'etc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###An example of quick to find unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['column']\n",
    "y.nunique() #this finds number of unique values\n",
    "y.unique() #this generates the contents of the array of unique values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mqVVAEXS4emO"
   },
   "source": [
    "###Rename the index of a dataframe or determine datatype of index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dzB63GWs4aja"
   },
   "outputs": [],
   "source": [
    "df.index.name = 'foo'\n",
    "type(df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Replace random null characters with numpy nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['column'] = df['column'].replace(\"?\",np.NaN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###From the great burrito data set in the sky. Use this to reassemble a bucket of favored labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's combine burrito categories to make this more useful\n",
    "df['Burrito'] = df['Burrito'].str.lower()\n",
    "\n",
    "california = df['Burrito'].str.contains('california')\n",
    "asada = df['Burrito'].str.contains('asada')\n",
    "surf = df['Burrito'].str.contains('surf')\n",
    "carnitas = df['Burrito'].str.contains('carnitas')\n",
    "\n",
    "df.loc[california, 'Burrito'] = 'California'\n",
    "df.loc[asada, 'Burrito'] = 'Asada'\n",
    "df.loc[surf, 'Burrito'] = 'Surf & Turf'\n",
    "df.loc[carnitas, 'Burrito'] = 'Carnitas'\n",
    "df.loc[~california & ~asada & ~surf & ~carnitas, 'Burrito'] = 'Other'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Datetime tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'] = pd.to_datetime(df['Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AfwOswQ_BQA7"
   },
   "source": [
    "### Drop rows based on index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5J2u2WlfBIPT"
   },
   "outputs": [],
   "source": [
    "df_array = np.array([0,3])\n",
    "df_frame = [[1,'a'],[2,'b'],[3,'c'],[4,'d']]\n",
    "df_frame = pd.DataFrame(df_frame)\n",
    "for each in (df_array):\n",
    "  df_frame.drop(index=[each],inplace=True)\n",
    "print(df_frame.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zAM_ihsLKJvn"
   },
   "source": [
    "###Setting maximum number of display rows when looking at head, list, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "60zugmg0KHDc"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 1567)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-y3LmIOAxJBa"
   },
   "source": [
    "###List Comprehension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wldvDDe8xOI5"
   },
   "outputs": [],
   "source": [
    "[col for col in df if col.endswith('_d')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZtttFDhCB8uL"
   },
   "source": [
    "###Covariance###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4VrYbL7wBTba"
   },
   "outputs": [],
   "source": [
    "# Python code to demonstrate the use of numpy.cov \n",
    "x = np.array([[0, 1], [1, 0]]) \n",
    "# Note this is a numpy array with a shape of 2x2. This works with all symmetric \n",
    "# dimensions. \n",
    "print(\"Shape of array:\\n\", np.shape(x)) \n",
    "print(\"Covariance matrix of x:\\n\", np.cov(x)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jbzMSQyQC_8i"
   },
   "source": [
    "### Dot Product ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AutpdZkdC_b0"
   },
   "outputs": [],
   "source": [
    "# input two matrices \n",
    "matrix1 = ([1, 6, 5],[3 ,4, 8],[2, 12, 3]) \n",
    "matrix2 = ([3, 4, 6],[5, 6, 7],[6, 56, 7]) \n",
    "# These are called from lists. They do not need to be symmetric. But they do \n",
    "# need to follow the inner / outer rule for rows & columns.\n",
    "# This will return dot product \n",
    "result = np.dot(matrix1,matrix2) \n",
    "# print resulted matrix \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HWPr584YGjN4"
   },
   "source": [
    "### Magnitude of a Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "muvjAeQvGmwj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "5AnCUC4hhtmA",
    "outputId": "c81f122f-c89d-49ed-b9da-b7e3b8f5af51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 3]\n",
      " [2 1]]\n",
      "[[-0.2  0.6]\n",
      " [ 0.4 -0.2]]\n",
      "[[ 1.00000000e+00 -5.55111512e-17]\n",
      " [ 0.00000000e+00  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1,3],[2,1]])\n",
    "print(x)\n",
    "y = np.linalg.inv(x)\n",
    "print(y)\n",
    "print(np.matmul(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9U5iEDen7DNK"
   },
   "outputs": [],
   "source": [
    "type(df['cats_allowed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "REOq6LJdK_ql"
   },
   "source": [
    "#Confusion Martrix Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JGJGfqY9LF-w"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "def my_confusion_matrix(y_true, y_pred):\n",
    "    labels = unique_labels(y_true)\n",
    "    columns = [f'Predicted {label}' for label in labels]\n",
    "    index = [f'Actual {label}' for label in labels]\n",
    "    table = pd.DataFrame(confusion_matrix(y_true, y_pred), \n",
    "                         columns=columns, index=index)\n",
    "    return sns.heatmap(table, annot=True, fmt='d', cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L-UzR58qUjqo"
   },
   "source": [
    "#Tracking Nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KCgFkLVTUrGs"
   },
   "source": [
    "Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JW4FUuyxUtBT"
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b9XmtOzeUvwF"
   },
   "outputs": [],
   "source": [
    "(df == 0).sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Split data for machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_test_split(train, train_size=0.80, test_size=0.20, \n",
    "                              stratify=train['status_group'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split on pandas datetime\n",
    "train = df[df['Date'].dt.year <= 2016]\n",
    "val = df[df['Date'].dt.year == 2017]\n",
    "test = df[df['Date'].dt.year >= 2018]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###List columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Determine Baseline of Classification Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have 2 classes, as expected, so this is binary classification\n",
    "# How do we check if the classes are imbalanced?\n",
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Classifer functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try a shallow decision tree as a fast, first model\n",
    "\n",
    "Make fast first models to clarify where you are with things.\n",
    "\n",
    "For classification problems:\n",
    "\n",
    "As a rough rule of thumb, if your majority class frequency is >= 50% and < 70% then you can just use accuracy if you want. Outside that range, accuracy could be misleading — so what evaluation metric will you choose, in addition to or instead of accuracy? For example:\n",
    "\n",
    "Precision?\n",
    "Recall?\n",
    "ROC AUC?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###ROC AUC\n",
    "Let's also review ROC AUC (Receiver Operating Characteristic, Area Under the Curve).\n",
    "\n",
    "Wikipedia explains, \"A receiver operating characteristic curve, or ROC curve, is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied. The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings.\"\n",
    "\n",
    "ROC AUC is the area under the ROC curve. It can be interpreted as \"the expectation that a uniformly drawn random positive is ranked before a uniformly drawn random negative.\"\n",
    "\n",
    "ROC AUC measures how well a classifier ranks predicted probabilities. So, when you get your classifier’s ROC AUC score, you need to use predicted probabilities, not discrete predictions.\n",
    "\n",
    "ROC AUC ranges from 0 to 1. Higher is better. A naive majority class baseline will have an ROC AUC score of 0.5, regardless of class (im)balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "y_pred_proba = pipeline.predict_proba(X_val)[:, -1] # Probability for the last class\n",
    "roc_auc_score(y_val, y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "target = 'Great'\n",
    "features = train.columns.drop([target, 'Date'])\n",
    "X_train = train[features]\n",
    "y_train = train[target]\n",
    "X_val = val[features]\n",
    "y_val = val[target]\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    ce.OrdinalEncoder(), \n",
    "    DecisionTreeClassifier(max_depth=3)\n",
    ")\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "print('Validation Accuracy', pipeline.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Quick decision tree graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "tree = pipeline.named_steps['decisiontreeclassifier']\n",
    "\n",
    "dot_data = export_graphviz(\n",
    "    tree, \n",
    "    out_file=None, \n",
    "    feature_names=X_train.columns, \n",
    "    class_names=y_train.unique().astype(str), \n",
    "    filled=True, \n",
    "    impurity=False,\n",
    "    proportion=True\n",
    ")\n",
    "\n",
    "graphviz.Source(dot_data)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DS Notes - Chris Filkins",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
